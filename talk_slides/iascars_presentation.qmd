---
title: "Seeing the smoke before the fire"
author: "Janith Wanniarachchi (Monash University)"
format:
  revealjs:
    theme: [default, iascars_presentation_theme.scss]
    slide-number: true
---

## Let's be honest here {.center}

. . . 

Building a good model is *hard*

. . . 

Explaining how a good model work is even **harder**

##  {.center .what-if}

. . . 

<img src="imgs/black_box_model-2.png" style = "width: 20%"/>
<br/>
<span style = "font-style: italic; font-size: 1.2rem"> Exhibit A: The good model </span>

. . . 

<div> **What if** you could <br/> poke around and find out <br/> how this model works?

. . . 

**Introducing**

<div class = "accent-color"> **Explainable AI (XAI) methods!** </div>


## XAI has a lot of facets {.smaller}

XAI can help you look at 

::::{.columns}

:::{.column width=50%}

<p style = "text-align:center"> <img src="imgs/global_methods_2.png" style="width:90%"/> </p>

How the model reacts to different features overall using <span class="accent-color"> Global Interpretability Methods </span>

::: {.callout-note}
## For example
For a model fitted to predict the probability of a bushfire ignition, these methods can tell us how a certain feature (e.g. temperature) affects the prediction overall.
:::

:::

:::{.column width=50%}

<p style = "text-align:center"> <img src="imgs/local_methods_2.png" style="width:90%"/> </p>

How the model gives a prediction to one single instance using <span class="accent-color">Local Interpretability Methods </span>

::: {.callout-note}
## For example
For a model fitted to predict the probability of a bushfire ignition, these methods can tell us given a specific test case and the associated model prediction, how and why the model came to that decision.
:::

::: 

::::

## Explaining one prediction

When it comes to understanding a model's decision process for a specific instance, there are several key methods that are related to each other in how they approach the problem

1. LIME (Local Interpretable Model agnostic Explanations)
2. Anchors
3. Counterfactuals

## How are these related {.smaller}

For a given data point 

:::{.incremental}
- LIME will tell you what features were important in giving that decision
  - Simply, it will fit a more understandable model that is as similar to the model as possible while also being explainable
- Counterfactuals will tell you how the data should change to sway the decision given the model.
  - Simply, it will try to find the closest instance to the given instance that has the prediction that is required while also being plausible
- Anchors will tell you the decision rules that the model followed for that instance and other similar instances
  - <span class="accent-color" style="font-weight: bold"> We will be talking about this today! </span>

:::

## What is an <span class = "what-is-a-obj"> Anchor </span>?

*Formally*

:::{.in-serif}

a rule or a set of predicates that satisfy the given instance and is a sufficient condition for $f(x)$ (i.e. the model output) with high probability

:::

<hr/>

:::{.callout-important}
## Simply put

We are going to find a big enough boundary box in the feature space containing other points that would have the same model prediction as the anchoring point.
:::

## What is a <span class = "what-is-a-obj"> predicate </span>?

Predicates are simple logical statements. In this context a predicate is made up of

:::: {.columns}
::: {.column width="20%"}
A feature 
<br/>

:::{.in-serif}
Age
:::

:::

::: {.column width="40%"}
A logical operator
<br/>

:::{.in-serif}
\>
:::

:::

::: {.column width="40%"}
A constant value
<br/>

:::{.in-serif}
42
:::

:::

::::

:::{.callout-important}
## Simply put

A predicate is a boundary line that divides a feature into two subsets.

:::

## How are Anchors made?

* This is achieved by formulating the problem as a Multi Armed Bandit problem to purely explore the feature space 

* We are going generate a large enough bounding box containing other points that would have the same model prediction as the anchoring point (i.e. higher coverage and precision)

## The Multi-arm Bandit problem

:::{.callout-important}
## Simply put

To put it simply imagine you are the local point and you are trying to make a wall of similar friends like yourself by changing the walls of the room. Your options are to either pull the north, east, west, or south walls to put all of your friends inside the walls. When you have lots of space you get rewarded and when you find like minded friends you get rewarded as well.

:::

## What is our reward

*Formally* the reward can be defined as follows,

$$
\begin{equation}
R(\mathcal{A}) =
    \begin{cases}
        \text{Prec}(\mathcal{A}) + \text{cov}(\mathcal{A})^2 & \text{if } \text{Prec}(\mathcal{A}) \in \mathbb{R} \\
        \text{Penalty} & \text{if } \text{Prec}(\mathcal{A}) < 0.6 
    \end{cases}
\end{equation}
$$

:::{.callout-important}
## Simply put,
We are going to be rewarded more for covering more ground while also being precise, and we are going to be punished if we push the walls into areas with weird precision values
:::

## Visualizing the way Anchors are built {.smaller}

Let's visualize how a multi armed bandit solution will try to find possible boundaries in two dimensional space for a given instance.

. . . 

- Watch as our solution navigates a 2D space, seeking boundaries.
- Observe the agent's journey through precision and coverage, trying to uncover the optimal solution.
- Compare the agent on different instances side-by-side


## Demonstration {.smaller}

* On the right is the boundary in the data space
* On the left the precision (x-axis) and coverage (y-axis) of the boundary is shown in bright red while the pink dots are the previously explored boundaries.

:::: {.columns}
::: {.column width=50%}

<p style="text-align:center"><img src="imgs/n_dim_state_plot_1.gif" style="width:90%"/></p>

<p class = "text-align:center"> <h4> Instance 1 </h4> </p>

:::

::: {.column width=50%}

<p style="text-align:center"><img src="imgs/n_dim_state_plot_1.gif" style="width:90%"/></p>

<p class = "text-align:center"> <h4> Instance 2 </h4> </p>

:::

::::

## What did we just observe {.smaller}

* The direction of the boundary box tells us the direction of the decision boundary
    * Blue class predictions → Lower right quadrant
    * Red class predictions → Upper left quadrant

. . . 

:::{.callout-tip}

In a practical setting we can redo this for multiple instances and get an understanding of the model's decision boundary and get an approximate understanding of the high and low feature values that affect a model prediction.

:::

# Applying XAI to bushfire modeling

## Why bushfires? {.smaller}

::::{.columns}

:::{.column width=70%}

* The Australian bushfires, notably during the 2019/2020 season, have emerged as a significant global concern, showcasing the escalating threat of climate change. 

* As bushfire ignitions become more frequent and intense even during the present year, building models to predict where the next fire is going to ignite is not enough. 

* By obtaining explanations from the model we can generate new insights into the key features and patterns to monitor to detect bushfires.

:::

:::{.column width=30%}

<p><img src="imgs/jo-anne-mcarthur-oitD52FFUu0-unsplash.jpg"/></p>

:::

::::



## Bushfire ignition data {.smaller}

::::{.columns}
:::{.column width=50%}
* Source: Department of Energy, Environment, and Climate Action
* Data Period: 2000-2021
* Data Type: Historic records of Victoria bushfires
* Collection Method: 000 calls and crew reports
* Objective : <span class="accent-color" style="font-weight:bold"> Build a model that can predict the cause of a possible bushfire and use explainable AI to uncover the decision process of the model </span>
:::

:::{.column width=50%}
* Observation Content:
  * Location
  * Date
  * Descriptive label of fire cause
* Label Refinement:
  * Refinement to 6 distinct causes:
      * Accidents
      * Arson
      * Burning off
      * Relight
      * Lightning
      * Other
:::
::::

## Gathering predictors 

- Weather data has been collected from SILO database through the weatherOz package.
  * Temperature
  * Rainfall
  * Humidity
  * Sunlight
- The distance to the nearest powerline for each bushfire ignition was also collected

## Fitting the model

:::{.callout-note}
## The problem statement is,

Given the location, the date, the weather and human activity data, predict the most likely cause for a bushfire.

:::

. . . 

As a baseline model, A Random forest model was fitted on a training set from 2000 to 2020 while the testing set contained data from 2021 to 2022 with an F1 score on the testing dataset of 0.83.

## How can XAI help in this scenario? {.smaller}

* From the two possible methods, local interpretability methods (which explains the decision process of one instance) are more useful in this setting

Given a models prediction of a bushfire cause for a given instance,

:::{.incremental}
- <span class = "accent-color"> LIME </span> can tell you, Which of the weather and human features were important in making this decision
- <span class = "accent-color"> Counterfactuals </span> can tell you, How should the weather and human factors be changed for this bushfire to be caused by another cause (according to the model's understanding)
- <span class = "accent-color"> Anchors </span> can tell you, What are the weather and human value ranges that the model considered for this instance and other instances with the same predicted cause
:::

## Visualizing Explanations {.smaller}

Each XAI method will give explanations which are inherently high dimensional as the model takes a lot of features to make predictors. 

The easiest to explain is anchors, which can given the decision rule that the model used to predict for the given instance and other similar instances. 

. . . 

::::{.columns}
:::{.column width=50%}

### Challenges

* Showing lower and upper bounds for multiple variables in a holistic manner is difficult as the data can be quite noisy
* Visualizing the model in data space and the boundary box given by anchor in high dimensional can be visually noisy to visualize.

:::

:::{.column width=50%}

### Possibilities

* Using grand tours to give an overall view of how the different variables interact in a high dimensional view
* Providing interactive controls for the user to tap into the model in data space and explore the decision boundary.

:::

::::

## Visualizing anchors {.smaller}

::::{.columns}
:::{.column width=50%}


<p style="text-align:center"><img src = "imgs/box_model.gif" /></p>


#### The discovered anchor and the class labels

:::

:::{.column width=50%}

Let's take the fitted bushfire model and use it to generate a likely decision boundary for a randomly selected lightning prediction.

Here the blue dots refer to the instances with the prediction lightning and the red dots are the instances that do not have lightning as a prediction.

Let's zoom in to the box and inspect it's contents.

:::
::::

---

::::{.columns}
:::{.column width=50%}

<p style="text-align:center"><img src = "imgs/inside_box_model.gif" /></p>


#### Local View

_A majority of the points in the box are lightning predictions_

:::

:::{.column width=50%}

<p style="text-align:center"><img src = "imgs/box_scatmat.gif" /></p>

#### Pairwise view

_There are clear clusters in the bivariate relationships indicating that there are relationships between the variables that would be heavily influential in building a model boundary._

:::
::::


## Conclusions

:::{.incremental}
* XAI is a growing field that is gaining continued development
* XAI methods can be divided into local or global methods based on the data that they explain
* LIME, Counterfactuals, and Anchors are interesting local methods
* Anchors can give easy to understand decision rules in the form of upper and lower boundaries
* Visualizing these explanations can be hard but there are promising results to advance forward.
:::

# Thank you!

